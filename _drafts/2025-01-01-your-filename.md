---
layout: post
---
## Deceptive Alignment Evals

In between travel and a full time job, I've been working my way through the [ARENA 3.0](https://www.arena.education/) course, in an attempt to upskill in the field of AI Safety. I started with [MLAB](https://www.redwoodresearch.org/mlab "(link is dead now)") content that was graciously shared with me by the folks at Redwood Research, and later realized that ARENA was a spiritual successor to the same. 

Chapter 3 of ARENA primarily concerns threat models and evaluations. It walks you through the process of thinking about and building a rudimentary threat model, concerning a single concerning property an AI model can exhibit. The course content gives us some examples of such properties:

> * Tendency to seek power
> * Sycophancy
> * Deceptive alignment (i.e. strategically overperforming on alignment eval tasks) 
> * Sandbagging (i.e. strategically underperforming on dangerous capabilities eval tasks)
> * Corrigibility with respect to a more/neuturally/less HHH (Honest, Helpful, and Harmless) goal
> * Desire for self-preservation
> * Non-myopia (far-sightedness) with respect to planning
> * Political bias

My property of choice was **deceptive alignment**, since I had just come across.